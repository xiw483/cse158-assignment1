{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import yaml\n",
    "import gzip\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"assignment1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "f = gzip.open(data_folder+\"train.json.gz\", 'r+')\n",
    "d = []\n",
    "for line in f:\n",
    "    value = eval(line)\n",
    "    d.append(value)\n",
    "data=pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks-Play Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-validation split\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "train = data.iloc[:165000]\n",
    "validation = data.iloc[165000:175000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary set of user/game pair indicating which games has each player played or not played\n",
    "played={}\n",
    "not_played={}\n",
    "user_id = train.userID.unique()\n",
    "game_id = train.gameID.unique()\n",
    "\n",
    "for user in user_id:\n",
    "    played[user] = []\n",
    "    not_played[user] = []\n",
    "\n",
    "for x in train.itertuples():\n",
    "    played[x.userID].append(x.gameID)\n",
    "\n",
    "for user in user_id:\n",
    "    not_played[user]= set(game_id) - set(played[user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-e4852ae54243>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  validation['played'] = True\n"
     ]
    }
   ],
   "source": [
    "#Create prediction label for validation set\n",
    "validation['played'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create negative samples\n",
    "neg=[]\n",
    "for x in validation.itertuples():\n",
    "    neg.append(random.sample(not_played[x.userID],1)[0])\n",
    "    \n",
    "validation_neg = pd.DataFrame()\n",
    "validation_neg['userID'] = validation.userID\n",
    "validation_neg['gameID'] = neg\n",
    "validation_neg['played'] = False\n",
    "\n",
    "validation = pd.concat([validation, validation_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary set of user/game pair indicating which users played each game\n",
    "game_user = {}\n",
    "for g in game_id:\n",
    "    game_user[g] = []\n",
    "\n",
    "for x in train.itertuples():\n",
    "    game_user[x.gameID].append(x.userID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let's try using Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Jaccard similarity\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define algorithm using the Jaccard similarity\n",
    "def jaccard_pred(u, g):\n",
    "    if u not in played.keys():\n",
    "        return 0\n",
    "    g1 = played[u]\n",
    "    if g in g1:\n",
    "        g1.remove(g)\n",
    "    s1 = set(game_user[g])\n",
    "    jmax = 0\n",
    "    for gi in g1:\n",
    "        s2 = set(game_user[gi])\n",
    "        j = Jaccard(s1, s2)\n",
    "        if j > jmax:\n",
    "            jmax = j\n",
    "    return jmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = {}\n",
    "\n",
    "gameCount = defaultdict(int)\n",
    "totalPlayed = 0\n",
    "\n",
    "for x in train.itertuples():\n",
    "    gameCount[x.gameID] += 1\n",
    "    totalPlayed += 1\n",
    "\n",
    "mostPopular = [(gameCount[x], x) for x in gameCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "counter = 0\n",
    "for t1 in np.linspace(1,3,5):\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    \n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        if count > totalPlayed/t1: break\n",
    "    \n",
    "    for t2 in np.linspace(0.01,0.05,5):\n",
    "        clear_output(wait=True)\n",
    "        pred = []\n",
    "        for x in validation.itertuples():\n",
    "            if (jaccard_pred(x.userID, x.gameID) > t2) and (x.gameID in return1):\n",
    "                pred.append(True)\n",
    "            else:\n",
    "                pred.append(False)\n",
    "        validation['pred'] = pred\n",
    "        accuracy[(t1,t2)] = (validation.pred == validation.played).sum()/len(validation.played)\n",
    "        print(np.round((counter/25)*100, 2), \"%\")\n",
    "        counter += 1\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best accuracy we got is when we set the threhold of popularity to __1.5__ and the threshold to similarity to __0.02__. This set up gives us an accuracy of __0.7054__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameCount = defaultdict(int)\n",
    "totalPlayed = 0\n",
    "\n",
    "for x in train.itertuples():\n",
    "    gameCount[x.gameID] += 1\n",
    "    totalPlayed += 1\n",
    "\n",
    "mostPopular = [(gameCount[x], x) for x in gameCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "return1 = set()\n",
    "count = 0\n",
    "    \n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalPlayed/1.5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.713\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for x in df.itertuples():\n",
    "    if (x.gameID in return1):\n",
    "        pred.append(True)\n",
    "    else:\n",
    "        pred.append(False)\n",
    "df['pred'] = pred\n",
    "accuracy = (df.pred == df.played).sum()/len(df.played)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Euclidean(s1,s2):\n",
    "    return len(s1.union(s2)) - len(s1.intersection(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Euclidean_pred(u, g):\n",
    "    if u not in played.keys():\n",
    "        return 0\n",
    "    g1 = played[u]\n",
    "    if g in g1:\n",
    "        g1.remove(g)\n",
    "    s1 = set(game_user[g])\n",
    "    jmax = 0\n",
    "    for gi in g1:\n",
    "        s2 = set(game_user[gi])\n",
    "        j = Euclidean(s1, s2)\n",
    "        if j > jmax:\n",
    "            jmax = j\n",
    "    return jmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for x in df.itertuples():\n",
    "    if (Euclidean_pred(x.userID, x.gameID) > 950):\n",
    "        pred.append(True)\n",
    "    else:\n",
    "        pred.append(False)\n",
    "df['pred'] = pred\n",
    "accuracy = (df.pred == df.played).sum()/len(df.played)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next up let's try Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use an adjusted cosine similarity where we take into consideration of hours played\n",
    "# First we construct a user-game pairwise metrics with hours_transformed as the value.\n",
    "temp = data[['gameID', 'userID']]\n",
    "data['played'] = 1\n",
    "M = data.pivot_table(columns='gameID', index='userID',values ='played')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first try a user-based recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_u = pd.DataFrame(cosine_similarity(\n",
    "    scale(M.fillna(-10000))),\n",
    "    index=M.index,\n",
    "    columns=M.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "played[user_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_pred_u(u,g):\n",
    "    if u not in played.keys():\n",
    "        return 0\n",
    "    u1 = game_user[g]\n",
    "    if u in u1:\n",
    "        u1.remove(u)\n",
    "    sim_max = 0\n",
    "    for ui in u1:\n",
    "        sim = similarity_u[u][ui]\n",
    "        if sim > sim_max:\n",
    "            sim_max = sim\n",
    "    return sim_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6664685714285714\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for x in data.itertuples():\n",
    "    if cosine_pred_u(x.userID, x.gameID) and (x.gameID in return1):\n",
    "        pred.append(True)\n",
    "    else:\n",
    "        pred.append(False)\n",
    "data['pred'] = pred\n",
    "accuracy = (data.pred == data.played).sum()/len(data.played)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next let's try item-based system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_i = pd.DataFrame(cosine_similarity(\n",
    "    scale(M.T.fillna(-10000))),\n",
    "    index=M.columns,\n",
    "    columns=M.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_pred_i(u,g):\n",
    "    if u not in played.keys():\n",
    "        return 0\n",
    "    g1 = played[u]\n",
    "    if g in g1:\n",
    "        g1.remove(g)\n",
    "    sim_max = 0\n",
    "    for gi in g1:\n",
    "        sim = similarity_i[g][gi]\n",
    "        if sim > sim_max:\n",
    "            sim_max = sim\n",
    "    return sim_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for x in df.itertuples():\n",
    "    if (cosine_pred_i(x.userID, x.gameID) > 0.03):\n",
    "        pred.append(True)\n",
    "    else:\n",
    "        pred.append(False)\n",
    "df['pred'] = pred\n",
    "accuracy = (df.pred == df.played).sum()/len(df.played)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for x in df.itertuples():\n",
    "    if (cosine_pred_u(x.userID, x.gameID) > 0.03) and (cosine_pred_i(x.userID, x.gameID) > 0) and (x.gameID in return1):\n",
    "        pred.append(True)\n",
    "    else:\n",
    "        pred.append(False)\n",
    "df['pred'] = pred\n",
    "accuracy = (df.pred == df.played).sum()/len(df.played)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for x in validation.itertuples():\n",
    "    if (cosine_pred_u(x.userID, x.gameID) > 0.03) and (cosine_pred_i(x.userID, x.gameID) > 0) and (x.gameID in return1):\n",
    "        pred.append(True)\n",
    "    else:\n",
    "        pred.append(False)\n",
    "validation['pred'] = pred\n",
    "accuracy = (validation.pred == validation.played).sum()/len(validation.played)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for x in validation.itertuples():\n",
    "    if (cosine_pred_u(x.userID, x.gameID) > 0.03) and (cosine_pred_i(x.userID, x.gameID) > 0.02) and (x.gameID in return1):\n",
    "        pred.append(True)\n",
    "    else:\n",
    "        pred.append(False)\n",
    "validation['pred'] = pred\n",
    "accuracy = (validation.pred == validation.played).sum()/len(validation.played)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for x in validation.itertuples():\n",
    "    if (cosine_pred_u(x.userID, x.gameID) > 0.05) and (cosine_pred_i(x.userID, x.gameID) > 0.02) and (x.gameID in return1):\n",
    "        pred.append(True)\n",
    "    else:\n",
    "        pred.append(False)\n",
    "validation['pred'] = pred\n",
    "accuracy = (validation.pred == validation.played).sum()/len(validation.played)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "test = pd.read_csv(data_folder+\"pairs_Played.txt\", sep=\"-|,\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary set of user/game pair indicating which games has each player played or not played\n",
    "played={}\n",
    "not_played={}\n",
    "user_id = data.userID.unique()\n",
    "game_id = data.gameID.unique()\n",
    "\n",
    "for user in user_id:\n",
    "    played[user] = []\n",
    "    not_played[user] = []\n",
    "\n",
    "for x in data.itertuples():\n",
    "    played[x.userID].append(x.gameID)\n",
    "\n",
    "for user in user_id:\n",
    "    not_played[user]= set(game_id) - set(played[user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_pred_u(u,g):\n",
    "    if u not in played.keys():\n",
    "        return 0\n",
    "    u1 = game_user[g]\n",
    "    if u in u1:\n",
    "        u1.remove(u)\n",
    "    sim_max = 0\n",
    "    for ui in u1:\n",
    "        sim = similarity_u[u][ui]\n",
    "        if sim > sim_max:\n",
    "            sim_max = sim\n",
    "    return sim_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_pred_i(u,g):\n",
    "    if u not in played.keys():\n",
    "        return 0\n",
    "    g1 = played[u]\n",
    "    if g in g1:\n",
    "        g1.remove(g)\n",
    "    sim_max = 0\n",
    "    for gi in g1:\n",
    "        sim = similarity_i[g][gi]\n",
    "        if sim > sim_max:\n",
    "            sim_max = sim\n",
    "    return sim_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validation.sample(frac=1)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.631"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gameCount = defaultdict(int)\n",
    "totalPlayed = 0\n",
    "\n",
    "for x in train.itertuples():\n",
    "    gameCount[x.gameID] += 1\n",
    "    totalPlayed += 1\n",
    "\n",
    "mostPopular = [(gameCount[x], x) for x in gameCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "    \n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalPlayed/3: break\n",
    "        \n",
    "pred = []\n",
    "for x in df.itertuples():\n",
    "    if ((cosine_pred_u(x.userID, x.gameID)>0.05) or (cosine_pred_i(x.userID,x.gameID)>0.05)) and (x.gameID in return1):\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(0)\n",
    "df['pred'] = pred\n",
    "np.sum(df.pred==df.played)/len(df.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameCount = defaultdict(int)\n",
    "totalPlayed = 0\n",
    "\n",
    "for x in train.itertuples():\n",
    "    gameCount[x.gameID] += 1\n",
    "    totalPlayed += 1\n",
    "\n",
    "mostPopular = [(gameCount[x], x) for x in gameCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "    \n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalPlayed/1.6: break\n",
    "    \n",
    "pred = []\n",
    "for x in test.itertuples():\n",
    "    if ((cosine_pred_u(x.userID, x.gameID)>0.05) or (cosine_pred_i(x.userID,x.gameID)>0.05)) and (x.gameID in return1):\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(0)\n",
    "test['prediction'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Played.txt\", 'w')\n",
    "predictions. truncate(0)\n",
    "predictions.write(\"userID-gameID,prediction\\n\")\n",
    "for x in test.itertuples():\n",
    "    predictions.write(str(x.userID) + '-' + str(x.gameID) + ',' + str(x.prediction) + '\\n')\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks-Category Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "f = gzip.open(data_folder+\"train_Category.json.gz\", 'r+')\n",
    "f.readline()\n",
    "d = []\n",
    "for line in f:\n",
    "    value = eval(line)\n",
    "    d.append(value)\n",
    "data=pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build set of all words appeared in the training data\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in train.itertuples():\n",
    "    r = ''.join([c for c in d.text.lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the 2500 most common words\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:5000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Bag of Words\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum.lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in train.text]\n",
    "y = train['genreID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build and fit logistic regression model\n",
    "logreg = LogisticRegression(max_iter=100000, C=1.5)\n",
    "logreg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions and evaludate the performance\n",
    "y_pred = logreg.predict([feature(d) for d in validation.text])\n",
    "acc = (y_pred == validation.genreID).sum()/len(y_pred)\n",
    "print(\"The accuracy of this model on the validation set is \", str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions and evaludate the performance\n",
    "y_pred = logreg.predict([feature(d) for d in train.text])\n",
    "acc = (y_pred == train.genreID).sum()/len(y_pred)\n",
    "print(\"The accuracy of this model on the validation set is \", str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_dict = {0:\"Action\",1:\"Strategy\",2:\"RPG\",3:\"Adventure\",4:\"Sports\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165000, 33629)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=20, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(train.text).toarray()\n",
    "labels = train.genreID\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_counts = CountVectorizer(stop_words='english',binary=False,min_df=5)\n",
    "X_counts=cv_counts.fit_transform(data.text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_Multinomial=MultinomialNB()\n",
    "clf_Multinomial.fit(X_counts,data.genreID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisReg=LogisticRegression(max_iter=100000, C=1.5)\n",
    "logisReg.fit(X_counts,data.genreID)\n",
    "# print('The train accuracy for Logistic Regression is {0}'.format(logisReg.score(X_train,y_train)))\n",
    "# print('The test accuracy for Logistic Regression is {0}'.format(logisReg.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved BOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "for d in data:\n",
    "    for w in d['review/text'].split():\n",
    "        w = ''.join([c for c in w.lower() if not c in punctuation])\n",
    "        w = stemmer.stem(w)\n",
    "        wordCount[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "words = [x[1] for x in counts[:3000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Bag of Words\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum.lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in train.text]\n",
    "y = train['genreID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build and fit logistic regression model\n",
    "logreg = LogisticRegression(max_iter=100000, C=1.5)\n",
    "logreg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions and evaludate the performance\n",
    "y_pred = logreg.predict([feature(d) for d in train.text])\n",
    "acc = (y_pred == train.genreID).sum()/len(y_pred)\n",
    "print(\"The accuracy of this model on the validation set is \", str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions and evaludate the performance\n",
    "y_pred = logreg.predict([feature(d) for d in validation.text])\n",
    "acc = (y_pred == validation.genreID).sum()/len(y_pred)\n",
    "print(\"The accuracy of this model on the validation set is \", str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load test data\n",
    "f = gzip.open(data_folder+\"test_Category.json.gz\", 'r+')\n",
    "d = []\n",
    "for line in f:\n",
    "    value = eval(line)\n",
    "    d.append(value)\n",
    "test=pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counts=cv_counts.transform(test.text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_Multinomial.predict(test_counts)\n",
    "test['prediction'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Category.txt\", 'w')\n",
    "predictions. truncate(0)\n",
    "predictions.write(\"userID-reviewID,prediction\\n\")\n",
    "for x in test.itertuples():\n",
    "    predictions.write(str(x.userID) + '-' + str(x.reviewID) + \",\" + str(x.prediction) + \"\\n\")\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(test_counts)\n",
    "test['prediction'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Category.txt\", 'w')\n",
    "predictions. truncate(0)\n",
    "predictions.write(\"userID-reviewID,prediction\\n\")\n",
    "for x in test.itertuples():\n",
    "    predictions.write(str(x.userID) + '-' + str(x.reviewID) + \",\" + str(x.prediction) + \"\\n\")\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
